{
 "metadata": {
  "name": "Profiling CPU and memory usage"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Monitoring memory usage\n",
      "\n",
      "When dealing with very large bioinformatics datasets, we might get worried about running out of memory. We can find out how much memory our program has used by importing the `resource` module and calling \n",
      "\n",
      "    resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
      "\n",
      "This will give us an answer in kilobyes, so normally we divide by 1000 to get MB.\n",
      "\n",
      "We can use this to investigate the behaviour of different bits of code in python. For example, how much more memory is used\n",
      "by a dict than a list?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import resource\n",
      "\n",
      "list = []\n",
      "\n",
      "# create a list with ten million elements\n",
      "\n",
      "for i in range(0,10000000):\n",
      "    list.append('abcdefg')\n",
      "    if len(list) % 1000000 == 0:\n",
      "        print(len(list), resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1000)\n",
      "\n",
      "# output:\n",
      "\n",
      "# 1000000 15.404\n",
      "# 2000000 23.248\n",
      "# 3000000 31.124\n",
      "# 4000000 38.78\n",
      "# 5000000 46.7\n",
      "# 6000000 54.568\n",
      "# 7000000 62.224\n",
      "# 8000000 70.144\n",
      "# 9000000 77.8\n",
      "# 10000000 85.72\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import resource\n",
      "\n",
      "\n",
      "# the same but with a dict\n",
      "dict = {}\n",
      "for i in range(0,10000000):\n",
      "    dict[i] = 'abcdefg'\n",
      "    if len(dict) % 1000000 == 0:\n",
      "        print(len(dict), resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1000)\n",
      "\n",
      "# output\n",
      "\n",
      "# 1000000 103.576\n",
      "# 2000000 199.88\n",
      "# 3000000 392.512\n",
      "# 4000000 392.512\n",
      "# 5000000 392.512\n",
      "# 6000000 777.728\n",
      "# 7000000 777.728\n",
      "# 8000000 777.728\n",
      "# 9000000 777.728\n",
      "# 10000000 777.728\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Interestingly, the dict uses 10 times more space to store roughly the same amount of information! When using a dict, \n",
      "we make an explicit trade off between memory and lookup speed. Remember, the 'goal' of a dict is to allow very fast\n",
      "lookup of values for keys even when the it gets very large. \n",
      "\n",
      "Another interesting difference is the way in which the memory usage grows. The size of a list grows linearly, whereas the \n",
      "size of a dict doubles, then stays the same for a while. This is an artefact of how dicts are stored internally - resizing\n",
      "the hash is a computationally expensive process, so Python tries to do it as rarely as possible\n",
      "\n",
      "Python is generally quite good at reclaiming unneeded space. For example, here is a script that creates two lists, each of \n",
      "ten million elements:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import resource\n",
      "\n",
      "list1 = []\n",
      "list2 = []\n",
      "for j in range(1,10):\n",
      "    for i in range(0,1000000):\n",
      "        list1.append('abcdefg')\n",
      "    print(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1000)\n",
      "\n",
      "for j in range(1,10):\n",
      "    for i in range(0,1000000):\n",
      "        list2.append('abcdefg')\n",
      "    print(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1000)\n",
      "\n",
      "# output:\n",
      "# 9.488\n",
      "# 13.508\n",
      "# 17.344\n",
      "# 21.304\n",
      "# 25.132\n",
      "# 29.092\n",
      "# 33.052\n",
      "# 37.012\n",
      "# 40.972\n",
      "# 44.872\n",
      "# 48.584\n",
      "# 52.544\n",
      "# 56.364\n",
      "# 60.324\n",
      "# 64.284\n",
      "# 68.244\n",
      "# 72.116\n",
      "# 76.076\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As expected, it uses roughly twice the memory of the earlier one. Look what happens when we empty the first list before creating the second one:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import resource\n",
      "\n",
      "list1 = []\n",
      "list2 = []\n",
      "for j in range(1,10):\n",
      "    for i in range(0,1000000):\n",
      "        list1.append('abcdefg')\n",
      "    print(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1000)\n",
      "# set the first list to be an empty list\n",
      "list1=[]\n",
      "for j in range(1,10):\n",
      "    for i in range(0,1000000):\n",
      "        list2.append('abcdefg')\n",
      "    print(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1000)\n",
      "\n",
      "# output\n",
      "# 9.488\n",
      "# 13.508\n",
      "# 17.344\n",
      "# 21.304\n",
      "# 25.132\n",
      "# 29.092\n",
      "# 33.052\n",
      "# 37.012\n",
      "# 40.972\n",
      "# 40.972\n",
      "# 40.972\n",
      "# 40.972\n",
      "# 40.972\n",
      "# 40.972\n",
      "# 40.972\n",
      "# 40.972\n",
      "# 40.972\n",
      "# 40.972\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Python is able to 'reclaim' the memory used by the first list, and re-use it for the second list, so the memory usage\n",
      "is constant in the second half of the script. (This process is known as [garbage collection](http://en.wikipedia.org/wiki/Garbage_collection_%28computer_science%29) and is a very interesting computer science problem in its own right).\n",
      "\n",
      "**Important** : the number that is reported by `getrusage` is the **maximum** amount of memory used - i.e. it does **not** tell you how much memory is being used right now, but the maximum over the lifetime of the program so far. Most of the time that is actually the number we are interested in, as it tells us how much ram we need to run a program with a particular dataset.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Profiling\n",
      "\n",
      "Profiling is the process of monitoring your code as it runs in order to figure out which bits of it take the most time. This\n",
      "is a vital step if we want to make a program run faster. In general, it is very hard to look at a program and guess\n",
      "which parts of it take the most time. There's no point trying to optimize a program before we know this - if a particular function only takes a tiny amount of the total run time, then we will be wasting our time trying to speed it up.\n",
      "\n",
      "How to profile your code: first import the `cProfile` module\n",
      "\n",
      "    import cProfile\n",
      "\n",
      "then write a function which executes your code, and call `cProfile.run()` with your function name as a string as its argument. \n",
      "\n",
      "    cProfile.run('myFunction()')\n",
      "\n",
      "Here is an example which uses the DNA -> protein translation code from the introductory course. This is a good candidate\n",
      "for profiling, because the code is split up into several functions:\n",
      "\n",
      "    split_into_codons\n",
      "    translate_codon\n",
      "    join_amino_acids\n",
      "    reverse_complement\n",
      "\n",
      "Any of which we could try to improve. Note that in the `run()` function we translate the same piece of DNA 10,000 times. This is because the translation function is already very quick - we want the script to run for at least a couple of seconds in order to get accurate profiling results, otherwise the results can be biased by overhead from starting/stopping the script.\n",
      "\n",
      "Profiling results can vary considerably between computers, operating systems, versions of Python, etc. so it's important to profile in an environment similar to the one where you will be running your program. This includes the input data - try to make your profiling runs as similar as possible to your real-life datasets. \n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cProfile\n",
      "\n",
      "gencode = {\n",
      "'ATA':'I','ACA':'T','AAC':'N','AGC':'S','CTA':'L','CCA':'P','CAC':'H','CGA':'R','GTA':'V','GCA':'A',\n",
      "'GAC':'D','GGA':'G','TCA':'S','TTC':'F','TAC':'Y','TGC':'C','ATC':'I','ACC':'T','AAT':'N','AGT':'S',\n",
      "'CTC':'L','CCC':'P','CAT':'H','CGC':'R','GTC':'V','GCC':'A','GAT':'D','GGC':'G','TCC':'S','TTT':'F',\n",
      "'TAT':'Y','TGT':'C','ATT':'I','ACG':'T','AAA':'K','AGA':'R','CTG':'L','CCG':'P','CAA':'Q','CGG':'R',\n",
      "'GTG':'V','GCG':'A','GAA':'E','GGG':'G','TCG':'S','TTA':'L','TAA':'_','TGA':'_','ATG':'M','ACT':'T',\n",
      "'AAG':'K','AGG':'R','CTT':'L','CCT':'P','CAG':'Q','CGT':'R','GTT':'V','GCT':'A','GAG':'E','GGT':'G',\n",
      "'TCT':'S','TTG':'L','TAG':'_','TGG':'W'}\n",
      "\n",
      "def translate_codon(codon):\n",
      "    return gencode.get(codon.upper(), 'x')\n",
      "\n",
      "def join_amino_acids(amino_acids):\n",
      "    result = ''\n",
      "    for aa in amino_acids:\n",
      "        result = result + aa\n",
      "    return result\n",
      "\n",
      "def split_into_codons(dna, frame):\n",
      "    codons = []\n",
      "    for i in range(abs(frame) - 1, len(dna)-2, 3):\n",
      "        codon = dna[i:i+3]\n",
      "        codons.append(codon)\n",
      "    return codons\n",
      "\n",
      "def reverse_complement(dna):\n",
      "    result_list = []\n",
      "    base_complement = {'a':'t', 't':'a', 'g':'c', 'c':'g'}\n",
      "    for base in dna:\n",
      "        result_list.append(base_complement[base])\n",
      "    result_list.reverse()\n",
      "    result_string = ''\n",
      "    for base in result_list:\n",
      "        result_string = result_string + base\n",
      "    return result_string\n",
      "\n",
      "def translate_dna_single(dna, frame=1):\n",
      "    if frame < 0:\n",
      "        dna = reverse_complement(dna)\n",
      "    codons = split_into_codons(dna, frame)\n",
      "    amino_acids = []\n",
      "    for codon in codons:\n",
      "        amino_acids.append(translate_codon(codon))\n",
      "    protein_string = join_amino_acids(amino_acids)\n",
      "    return protein_string\n",
      "\n",
      "def translate_dna(dna):\n",
      "    all_translations = []\n",
      "    for frame in range(1,4):\n",
      "        all_translations.append(translate_dna_single(dna, frame))\n",
      "    for frame in range(-4,-1):\n",
      "        all_translations.append(translate_dna_single(dna, frame))\n",
      "    return all_translations\n",
      "\n",
      "def run():\n",
      "    for i in range(0,10000):\n",
      "        dna = 'atgcgatcgatcgatcgatgctagctacgtagcatcgatc'\n",
      "        translate_dna(dna)\n",
      "\n",
      "cProfile.run('run()')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "         5380004 function calls in 7.063 seconds\n",
        "\n",
        "   Ordered by: standard name\n",
        "\n",
        "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
        "   750000    1.378    0.000    2.257    0.000 <ipython-input-1-b95e6a946fed>:12(translate_codon)\n",
        "    60000    0.121    0.000    0.121    0.000 <ipython-input-1-b95e6a946fed>:15(join_amino_acids)\n",
        "    60000    0.761    0.000    1.251    0.000 <ipython-input-1-b95e6a946fed>:21(split_into_codons)\n",
        "    30000    0.990    0.000    1.667    0.000 <ipython-input-1-b95e6a946fed>:28(reverse_complement)\n",
        "    60000    1.158    0.000    6.875    0.000 <ipython-input-1-b95e6a946fed>:39(translate_dna_single)\n",
        "    10000    0.140    0.000    7.051    0.001 <ipython-input-1-b95e6a946fed>:49(translate_dna)\n",
        "        1    0.012    0.012    7.063    7.063 <ipython-input-1-b95e6a946fed>:57(run)\n",
        "        1    0.000    0.000    7.063    7.063 <string>:1(<module>)\n",
        "    60000    0.034    0.000    0.034    0.000 {built-in method abs}\n",
        "        1    0.000    0.000    7.063    7.063 {built-in method exec}\n",
        "    60000    0.035    0.000    0.035    0.000 {built-in method len}\n",
        "  2760000    1.537    0.000    1.537    0.000 {method 'append' of 'list' objects}\n",
        "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
        "   750000    0.451    0.000    0.451    0.000 {method 'get' of 'dict' objects}\n",
        "    30000    0.018    0.000    0.018    0.000 {method 'reverse' of 'list' objects}\n",
        "   750000    0.428    0.000    0.428    0.000 {method 'upper' of 'str' objects}\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The output from cProfile tells us some stats for each function:\n",
      "\n",
      "ncalls is the total number of times the function was called. As we expect, `translate_codon` is the function that is called\n",
      "the most times. \n",
      "\n",
      "tottime is the total amount of time spent in the function, excluding subfunctions. This is probably the most important stat. \n",
      "Functions that take up a large total time are the best ones to try to speed up, as they will have the most effect on the overall program speed.\n",
      "\n",
      "percall is the time it takes to run the function once, i.e. the total time divided by the number of calls. \n",
      "\n",
      "cumtime is the cumulative time to run the function, including all subfunctions. \n",
      "\n",
      "The second field called percall the same as the first, but including subfunctions. \n",
      "\n",
      "Looking at the results from the above run, we see that `translate_codon` has the largest total time with 1.4 seconds. However, this function is so simple that it's not really obvious how we could speed it up! The next function is `reverse_complement` with just under 1 second. This is quite a complicated function, so we could experiment with different\n",
      "ways of writing it in order to speed it up. \n",
      "\n",
      "Interestingly, the program spends a long time in the `list.append()` method that is built-in to Python. We cannot try to speed this up (it is probably very efficient already, since it is a core part of the Python language) but maybe we could try to rewrite our code to use fewer list append operations. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's look at another example - this time we will use the sequence similarity search program from day one. Again, we will write a `run()` method that carries out the same search many times, in order to get an accurate reading."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cProfile\n",
      "def get_words(sequence, wordsize):\n",
      "    result = {}\n",
      "    for pos in range(0,len(sequence) + 1 - wordsize):\n",
      "        word = sequence[pos:pos+wordsize]\n",
      "        current_list = result.get(word, [])\n",
      "        current_list.append(pos)\n",
      "        result[word] = current_list\n",
      "    return result\n",
      "\n",
      "\n",
      "def score_match(subject, query, subject_start, query_start, length):\n",
      "    score = 0\n",
      "    for i in range(0,length):\n",
      "        subject_base = subject[subject_start + i]\n",
      "        query_base = query[query_start + i]\n",
      "        if subject_base == query_base:\n",
      "            score = score + 1\n",
      "        else:\n",
      "            score = score - 1\n",
      "    return score\n",
      "\n",
      "\n",
      "def pretty_print_match(subject, query, subject_start, query_start, length):\n",
      "    result = ''\n",
      "    result += str(subject_start) + (' ' * length) + str(subject_start+length) + '\\n'\n",
      "    result += ' ' + subject[subject_start:subject_start+length] + '\\n'\n",
      "    result += ' ' + query[query_start:query_start+length] + '\\n'\n",
      "    result += str(query_start) + (' ' * length) + str(query_start+length) + '\\n'\n",
      "    result += '\\n--------------------\\n' + '\\n'\n",
      "    return result\n",
      "\n",
      "def find_matches(subject, query, wordsize, score_threshold):\n",
      "    subject_words = get_words(subject, wordsize)\n",
      "    all_matches = []\n",
      "    for query_pos in range(0,len(query) + 1 - wordsize):\n",
      "        word = query[query_pos:query_pos+wordsize]\n",
      "        for subject_pos in subject_words.get(word, []):\n",
      "            all_matches.append(extend_hit(subject, query, subject_pos, query_pos, wordsize, score_threshold))\n",
      "    result = ''\n",
      "    for match in sorted(all_matches, key=lambda x : x['score'], reverse=True):\n",
      "        result += 'Score : ' + str(match['score']) + '\\n'\n",
      "        result += pretty_print_match(match['subject'], match['query'], match['subject_pos'], match['query_pos'], match['length'])       \n",
      "    return result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extend_hit(subject, query, subject_pos, query_pos, length, score_threshold):\n",
      "    while length < len(subject) and length < len(query):\n",
      "        # if the current length is even, then extend to the right, else extend to the left\n",
      "        # this just makes sure that we try left and right alternately\n",
      "        if length %2 == 0:\n",
      "            if len(query) <= query_pos + length + 1 or len(subject) <= subject_pos + length + 1:\n",
      "                return {\n",
      "                        'subject' : subject, 'query' : query, 'subject_pos' : subject_pos, 'query_pos' : query_pos, 'length' : length,\n",
      "                        'score' : score_match(subject, query, subject_pos, query_pos, length)\n",
      "                        }\n",
      "            if score_match(subject, query, subject_pos, query_pos, length+1) < score_threshold:\n",
      "                return {\n",
      "                        'subject' : subject, 'query' : query, 'subject_pos' : subject_pos, 'query_pos' : query_pos, 'length' : length,\n",
      "                        'score' : score_match(subject, query, subject_pos, query_pos, length)\n",
      "                        }\n",
      "            else:\n",
      "                length = length + 1\n",
      "        else:\n",
      "            # if either the subject or query pos is already 0, then we cannot extend to the left\n",
      "            if query_pos == 0 or subject_pos == 0:\n",
      "                return {\n",
      "                        'subject' : subject, 'query' : query, 'subject_pos' : subject_pos, 'query_pos' : query_pos, 'length' : length,\n",
      "                        'score' : score_match(subject, query, subject_pos, query_pos, length)\n",
      "                        }\n",
      "            # to test the new score we need to reduce the start positions by one\n",
      "            if score_match(subject, query, subject_pos-1, query_pos-1, length+1) < score_threshold:\n",
      "                return {\n",
      "                        'subject' : subject, 'query' : query, 'subject_pos' : subject_pos, 'query_pos' : query_pos, 'length' : length,\n",
      "                        'score' : score_match(subject, query, subject_pos, query_pos, length)\n",
      "                        }\n",
      "            else:\n",
      "            # if the extended match is OK, then make the changes\n",
      "                length = length + 1\n",
      "                subject_pos = subject_pos-1\n",
      "                query_pos = query_pos-1\n",
      "      \n",
      "\n",
      "def run():\n",
      "    for i in range(0,10000):\n",
      "        find_matches('actgatcgtagcaatcgatcgatgcatctcgagcttcatacgacgatgctacgtacga', 'ttattcatctccagaa', 4, 2)\n",
      "\n",
      "cProfile.run('run()')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "         3200004 function calls in 5.886 seconds\n",
        "\n",
        "   Ordered by: standard name\n",
        "\n",
        "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
        "   440000    1.414    0.000    1.414    0.000 <ipython-input-2-29901b4a61d2>:12(score_match)\n",
        "    10000    0.892    0.000    1.544    0.000 <ipython-input-2-29901b4a61d2>:2(get_words)\n",
        "    50000    0.220    0.000    0.220    0.000 <ipython-input-2-29901b4a61d2>:24(pretty_print_match)\n",
        "    10000    0.390    0.000    5.820    0.001 <ipython-input-2-29901b4a61d2>:33(find_matches)\n",
        "    50000    0.031    0.000    0.031    0.000 <ipython-input-2-29901b4a61d2>:41(<lambda>)\n",
        "    50000    1.356    0.000    3.447    0.000 <ipython-input-3-3de2c95a906a>:1(extend_hit)\n",
        "        1    0.066    0.066    5.886    5.886 <ipython-input-3-3de2c95a906a>:38(run)\n",
        "        1    0.000    0.000    5.886    5.886 <string>:1(<module>)\n",
        "        1    0.000    0.000    5.886    5.886 {built-in method exec}\n",
        "  1300000    0.688    0.000    0.688    0.000 {built-in method len}\n",
        "    10000    0.071    0.000    0.101    0.000 {built-in method sorted}\n",
        "   600000    0.353    0.000    0.353    0.000 {method 'append' of 'list' objects}\n",
        "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
        "   680000    0.404    0.000    0.404    0.000 {method 'get' of 'dict' objects}\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we find that two functions are using up the majority of the time - `score_match` and `extend_hit`.  Let's try to \n",
      "speed up `extend_hit`. Look at the first few lines of the function:\n",
      "\n",
      "    def extend_hit(subject, query, subject_pos, query_pos, length, score_threshold):\n",
      "        while length < len(subject) and length < len(query):\n",
      "            if length %2 == 0:\n",
      "                if len(query) <= query_pos + length + 1 or len(subject) <= subject_pos + length + 1:\n",
      "\n",
      "We are calling `length(subject)` and `length(query)` a total of four times each time round the `while` loop. We know that the\n",
      "subject sequence and query sequence are not going to change inside this function so let's store the length in variables then\n",
      "we only have to calculate them once:\n",
      "\n",
      "\n",
      "    def extend_hit(subject, query, subject_pos, query_pos, length, score_threshold):\n",
      "        subject_length = len(subject)\n",
      "        query_length = len(query)\n",
      "        while length < subject_length and length < query_length:\n",
      "        if length %2 == 0:\n",
      "            if query_length <= query_pos + length + 1 or subject_length <= subject_pos + length + 1:\n",
      "\n",
      "Now we only make a two calls to `len()` in the whole function. How does this affect the profiling results? "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    Ordered by: standard name\n",
      "\n",
      "     ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "          1    0.000    0.000    4.551    4.551 <string>:1(<module>)\n",
      "     440000    1.449    0.000    1.449    0.000 speedtest2.py:12(score_match)\n",
      "      10000    0.878    0.000    1.520    0.000 speedtest2.py:2(get_words)\n",
      "      50000    0.214    0.000    0.214    0.000 speedtest2.py:24(pretty_print_match)\n",
      "      10000    0.366    0.000    4.501    0.000 speedtest2.py:33(find_matches)\n",
      "      50000    0.030    0.000    0.030    0.000 speedtest2.py:41(<lambda>)\n",
      "      50000    0.678    0.000    2.182    0.000 speedtest2.py:47(extend_hit)\n",
      "          1    0.050    0.050    4.551    4.551 speedtest2.py:84(run)\n",
      "          1    0.000    0.000    4.551    4.551 {built-in method exec}\n",
      "     120000    0.067    0.000    0.067    0.000 {built-in method len}\n",
      "      10000    0.071    0.000    0.101    0.000 {built-in method sorted}\n",
      "     600000    0.341    0.000    0.341    0.000 {method 'append' of 'list' objects}\n",
      "          1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "     680000    0.407    0.000    0.407    0.000 {method 'get' of 'dict' objects}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have reduced the total length of time spent in `extend_hit` by almost half, and reduced the number of calls to the built-in function `len()` from 1.3 million to 120,000."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    }
   ],
   "metadata": {}
  }
 ]
}